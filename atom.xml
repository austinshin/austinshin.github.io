<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Austin Shin]]></title>
  <link href="http://austinshin.github.io/atom.xml" rel="self"/>
  <link href="http://austinshin.github.io/"/>
  <updated>2018-02-26T13:46:48-08:00</updated>
  <id>http://austinshin.github.io/</id>
  <author>
    <name><![CDATA[Austin Shin]]></name>
    <email><![CDATA[shinaustin@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[tic-tac-toe cli]]></title>
    <link href="http://austinshin.github.io/blog/2018/02/22/tic-tac-toe/"/>
    <updated>2018-02-22T14:16:49-08:00</updated>
    <id>http://austinshin.github.io/blog/2018/02/22/tic-tac-toe</id>
    <content type="html"><![CDATA[<p>Created a tic-tac-toe command line game to practice coding under time constraints and practice using node&rsquo;s readline.</p>

<p>Find it here:</p>

<p><a href="https://github.com/austinshin/tictactoe-cli">tictactoe</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[blackjack]]></title>
    <link href="http://austinshin.github.io/blog/2018/02/22/blackjack/"/>
    <updated>2018-02-22T14:16:41-08:00</updated>
    <id>http://austinshin.github.io/blog/2018/02/22/blackjack</id>
    <content type="html"><![CDATA[<p>Created a blackjack game run through command line to practice quick programming and object-oriented programming. First experience with node&rsquo;s readline.</p>

<p>Find it here:</p>

<p><a href="https://github.com/austinshin/blackjack-cli">blackjack</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[docker and ec2]]></title>
    <link href="http://austinshin.github.io/blog/2018/02/13/docker-and-ec2/"/>
    <updated>2018-02-13T19:53:07-08:00</updated>
    <id>http://austinshin.github.io/blog/2018/02/13/docker-and-ec2</id>
    <content type="html"><![CDATA[<h1>A Docker And EC2 Instance Guide</h1>

<p>-</p>

<p><strong>This is a bare bones guide to setting up a project on an EC2 instance</strong></p>

<p>Things this guide will cover and hopefully answer:</p>

<ol>
<li>What is docker?</li>
<li>What is an EC2 Instance?</li>
<li>How do I tie them together with my app that I am developing locally?</li>
<li>How do I set it up?</li>
</ol>


<p>-</p>

<!--more-->


<p><strong>Disclaimer:</strong>
This is a naive method. It does not use ECS, load balancing, clusters all that stuff. You can look that up (and its very confusing tutorials which I thoroughly browsed) if you wish.</p>

<p>If you have a two databases and one node app (i.e redis, mongodb, nodejs files) you should end up with 3 ec2 instances. (1 minimum dockerized container or 3 depending on how you do it).</p>

<p>There is an even more naive way to do it which is to just use docker-compose on all 3 and put them in one ec2-instance (if you got docker-compose to work on your local machine you are basically doing the same thing on your ec2 instance).</p>

<p><strong>I suggest skimming through the guide to get a good understanding of how this works before blindly following it (although I believe I covered most of the essentials). YOU MIGHT HAVE TO GOOGLE SOME STUFF in addition.</strong></p>

<p>-</p>

<h1>Create an EC2 Instance.</h1>

<ol>
<li>Follow this guide
<a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/get-set-up-for-amazon-ec2.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/get-set-up-for-amazon-ec2.html</a></li>
<li>Sign up for AWS (make an account/pw), create an IAM user, create a key pair.</li>
<li><strong>Save your key pair to your local machine. You will use it. (It should be a .pem file) Mine will be called app.pem for reference.</strong></li>
<li>MAKE SURE YOU ARE IN THE <strong>CORRECT REGION</strong> YOU WANT TO BE IN</li>
<li>(select in top-right us-west-1 N. California).
<img src="https://puu.sh/znR7s/f6c593d6fd.png" alt="ss" /></li>
<li><p>You do not have to create a VPC or security group.</p></li>
<li><p>In you AWS console, click the top left Services button and then EC2 (make sure correct region always)</p></li>
<li>Click Launch Instance.</li>
<li>Select type of machine image you want to launch. I suggest one of these.
They require different commands depending on the one you choose.

<ul>
<li>Amazon Linux AMI 2017.09.1 (HVM), SSD Volume Type</li>
<li>Ubuntu Server 16.04 LTS (HVM), SSD Volume Type - ami-07585467</li>
</ul>
</li>
<li>Select type of instance you want.

<ul>
<li>t2.micro is free tier eligible</li>
<li>Other ones you have to pay more depending on how long they are running for.</li>
<li>Bigger ones give more space or are &lsquo;stronger&rsquo; cpus.</li>
</ul>
</li>
<li>Select default for everything for simplicity sake
(you can specifiy and customize as you understand/research more).</li>
<li>Step 4 (Add storage), 5 (Add tags) leave default.</li>
<li><p>Step 6 - Configure security group</p>

<ul>
<li>Make sure you have something like this for your security group.
<img src="https://puu.sh/znQGT/affbd06e61.png" alt="ss" /></li>
<li>This is super important because you need to have the inbound port 22 allowed
so you can access it from the outside world (which is what SSH uses).</li>
<li>Another example: You want to access your service from the outside world on
port 80. So what you would do is add another inbound rule allowing port 80
of a specific ip (or anywhere).</li>
<li>The figure below allows access on port 80, 22, 6379, 7474, 3000, 7687 on anywhere.
<img src="https://puu.sh/znQZ2/8851b026d4.png" alt="ss" /></li>
<li>Add the ports you need depending on the database your port exposes to.</li>
</ul>
</li>
<li><p>Review and Launch and wait for it to set up.</p></li>
<li>You should see something like this now in Services -> Instances.
<img src="https://puu.sh/znRam/44bfb0821a.png" alt="ss" /></li>
<li>Click it. At the bottom you should see a public IPv4. This is the IP you can access your instance from.</li>
<li>We now need to ssh into it.</li>
</ol>


<p>On your terminal (omit the $ from all your commands you c/p).</p>

<p>Find your pem file (mine is app.pem) and is located on desktop. You can move it to your
~/.ssh folder if you wish.
Do 1 of these.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>example: $ chmod 400 &lt;PATH TO pem file&gt;
</span><span class='line'>
</span><span class='line'>$ chmod 400 ~/desktop/app.pem        &lt;- (if it located in desktop)
</span><span class='line'>$ chmod 400 ~/.ssh/app.pem           &lt;- (if it is located in .ssh)</span></code></pre></td></tr></table></div></figure>


<p>Now we ssh in using this template</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>example: $ ssh -i &lt;PATH TO pem file&gt; &lt;type of machine image&gt;@&lt;public ip of YOUR ec2 instance&gt;
</span><span class='line'>
</span><span class='line'>$ ssh -i ~/desktop/app.pem ec2-user@57.213.217.136     &lt;- use this if you created a linux machine image from step 4.
</span><span class='line'>$ ssh -i ~/desktop/app.pem ubuntu@57.213.217.136       &lt;- use this if you created an ubuntu machine image from step 4.
</span></code></pre></td></tr></table></div></figure>


<h4>Success your ec2 instance is now created!</h4>

<h2>This part below is optional. Only do this if you DON&rsquo;T want to use docker. Skip to docker otherwise.</h2>

<h3>Installing a Database/application to your ec2 instance.</h3>

<p>Now you should have access to your ec2 instance. You can run commands on it and do whatever you wish. The best way to think about your ec2-instance is that it is now your new &lsquo;local computer&rsquo;. You can install any type of database you wish i.e. mongodb, redis, neo4j, cassandra, postgres, mysql, etc. How do I do this?</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Google this: "How to install &lt; DB NAME &gt; on &lt; MachineImageUsedType &gt; EC2 Instance"</span></code></pre></td></tr></table></div></figure>


<p>i.e. &ldquo;How to install redis on linux ec2 instance&rdquo; links me to this <a href="https://medium.com/@andrewcbass/install-redis-v3-2-on-aws-ec2-instance-93259d40a3ce">article</a>.</p>

<p>Follow the steps! Some of the links will lead you to the official manual to set you up.
Each database has its own specific quirk to installing it on Ubuntu or Linux just like own OSX or Windows. You have to figure that part out.</p>

<p>Note: installation of certain databases (like redis) is super simple.</p>

<p>Installation of certain databases (like cassandra) is really annoying because you have to configure yml files and whatnot. Look at the section below if you want to skip this pain.</p>

<p><strong>Assuming you decided to not use docker and you installed your specific database.</strong></p>

<p>Ideally when you are sshed into your EC2 Instance you should be able to run something like if you have your db properly installed.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ redis-cli
</span><span class='line'>$ cypher-shell
</span><span class='line'>$ MYSQL -u root
</span><span class='line'>$ cqlsh</span></code></pre></td></tr></table></div></figure>


<p><strong>If you properly installed it on your EC2 instance, move onto the exposing port to real world</strong></p>

<p>Hopefully it worked!</p>

<p>Final note for this section: You can also deploy your regular applications this way too! Say your MVP project. You could git clone your files from the ec2 instance, spin it up, install a package that has node run detached, expose port 3000 (or whatever port you exposed in express), and you now have access to that app from the real world!</p>

<p>-</p>

<h1>Docker</h1>

<p>There is an issue with installing databases and applications on an EC2 instance.
For example, installing cassandra on OSX is a pain in the ass. You have to make sure you have the specific Java version (8 with a specific tag)&hellip; You have to install all these different dependencies. Some of these dependencies change according to the machine you have. Imagine trying to install it on some new interface (Linux,Ubuntu) you&rsquo;ve never dealt with.</p>

<p><strong>I wish I could do something like a npm install cassandra on this EC2 instance and magically make it for me.</strong></p>

<p><strong>Here is where docker comes in.</strong></p>

<p>You can skip all the previous installation steps in the section above and trying to figure out how to install and get the correct packages on your linux/ubuntu instance and just do this.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>1. Install docker
</span><span class='line'>2. Pull the docker database image
</span><span class='line'>3. Run the docker image (creating a new container) on a specific port
</span><span class='line'>4. Expose the port via the inbound security group
</span><span class='line'>5. You're done!</span></code></pre></td></tr></table></div></figure>


<p>Docker is an OS virtualization within a computer which provides a layer of abstraction. This means all of the apps you want to build will always build equally across different types of machines. Imagine if you were building an application on OSX, but randomly decided to change operating systems to Windows. Instead of reinstalling everything on Windows, you can just install docker, create a docker image of your app, pull it to your windows, then continue to work off that. It enables <strong>consistency across environments.</strong></p>

<p>-</p>

<h1>Using Docker</h1>

<h3>If you are using a Linux machine image from step 4.</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>YourTerminal-$ ssh -i ~/desktop/app.pem ec2-user@57.213.217.136
</span><span class='line'>
</span><span class='line'>You should be sshed into your ec2-instance now (type yes).
</span><span class='line'>
</span><span class='line'>ec2-linux-$ sudo yum update                           &lt;- updates linux
</span><span class='line'>ec2-linux-$ sudo yum install -y docker                &lt;- installs docker
</span><span class='line'>ec2-linux-$ sudo service docker start                 &lt;- starts docker
</span><span class='line'>ec2-linux-$ sudo usermod -a -G docker ec2-user        &lt;- no longer need sudo for docker commands
</span><span class='line'>ec2-linux-$ docker info                               &lt;- docker info</span></code></pre></td></tr></table></div></figure>


<h3>If you are using an Ubuntu machine image from step 4.</h3>

<p>Commands are slightly different.</p>

<p>Referenced from <a href="https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-on-ubuntu-16-04">https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-on-ubuntu-16-04</a></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>YourTerminal-$ ssh -i ~/desktop/app.pem ubuntu@57.213.217.136
</span><span class='line'>ec2-ubuntu-$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
</span><span class='line'>ec2-ubuntu-$ sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
</span><span class='line'>ec2-ubuntu-$ sudo apt-get update
</span><span class='line'>ec2-ubuntu-$ apt-cache policy docker-ce
</span><span class='line'>ec2-ubuntu-$ sudo apt-get install -y docker-ce
</span><span class='line'>ec2-ubuntu-$ sudo systemctl status docker</span></code></pre></td></tr></table></div></figure>


<p>Great. We now have docker installed on our ec2 instance. What&rsquo;s next? It&rsquo;s very simple. All we have to do is figure out what image from dockerhub to pull down depending on our database.</p>

<p>Go here -> <a href="https://hub.docker.com/explore/">dockerhub</a></p>

<p>You can see all types of pre built official images like node, redis, mongo, mysql, etc.</p>

<p>Let&rsquo;s say we want to spin up a redis container.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Make sure docker is running.
</span><span class='line'>ec2-instance-$ docker run --name new-redis-instance -d -p 6379:6379 redis
</span></code></pre></td></tr></table></div></figure>


<p>If you have enough memory on your ec2 instance and it can support the database, it should work.</p>

<p>The run command looks for an image name and pulls from dockerhub if one is not stored locally. It uses the image to spin up a container of the application you specified (the image).</p>

<p>The -d flag creates the container in detached mode. Meaning you can just exit out of your ec2 instance and it will still be running. The first port is the one it exposes to the outside, and the second port is the one for the inside world.</p>

<p>The &ndash;name flag is to specify the name.</p>

<p>The last &lsquo;redis&rsquo; is just the image name. So if I wanted to use the mongo image I&rsquo;d use &lsquo;mongo&rsquo; instead. If I wanted to specify a specific version, I could add a tag like so.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>ec2-instance-$ docker run --name new-redis-instance -d -p 6379:6379 redis:alpine</span></code></pre></td></tr></table></div></figure>


<p>Useful docker commands</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ docker ps -a                       &lt;- shows all containers running/stopped
</span><span class='line'>$ docker stop 'container name/id'       &lt;- specify the container you wish to stop.
</span><span class='line'>$ docker rm 'container name/id'         &lt;- remove the container you wish (this destroys data unless you specified a volume)
</span><span class='line'>$ docker images                      &lt;- shows all images you've downloaded locally (cached)
</span><span class='line'>$ docker rmi 'image name/id'         &lt;- deletes the image
</span><span class='line'>$ docker network ls                  &lt;- shows all docker networks
</span><span class='line'>$ docker inspect 'specific network i.e bridge'    &lt;- shows all docker containers on that network (useful for docker-compose)
</span><span class='line'>$ docker exec -it 'container name' sh  &lt;- ssh into docker container shell.
</span></code></pre></td></tr></table></div></figure>


<p>To make sure it is working&hellip; do this next! If your container is immediately crashing it might be a memory issue (your service isn&rsquo;t capable enough to handle it). But for mongo, redis, a micro should be enough (until you upload millions of data).</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>ec2-instance-$ docker ps -a     &lt;- make sure your docker container is working (for the db!)
</span><span class='line'>ec2-instance-$ docker exec -it 'container name' sh        &lt;- sshs you into the shell.</span></code></pre></td></tr></table></div></figure>


<p>You should now be in your docker container and be able to browse the contents. Essentially you are now inside of a computer inside of a computer&hellip; pretty cool!
Try running the cli for your specific database. So, if you created a redis db you should be able to do:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>docker-container-$ redis-cli</span></code></pre></td></tr></table></div></figure>


<p>And it should work. This means your db inside your docker is up and running and if you ran the specific port, it is exposing it outside to ec2 (via 6379) when you did docker run -p 6379:6379.</p>

<p>The last step is to now expose your 6379 port to the outside world. In otherwords, have other services be able to talk to it. Follow the steps in &ldquo;Exposing port to the real world for EC2 Instance&rdquo;</p>

<p>-</p>

<h1>Exposing port to the real world for EC2 Instances.</h1>

<p>These next few steps apply to whether you are using docker or no docker (only for AWS).
The next step is to make sure you have the port you can access your db to expose. So if you use redis, the default port it exposes the database to something like 6379 i.e localhost:6379.</p>

<p>So for example if I created an EC2 Instance and I wanted to access the redis db on the ec2 instance from locally (your computer).</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>I should be able to access my EC2 Instance from Public ip: 54.241.150.234:6379 right?</span></code></pre></td></tr></table></div></figure>


<p><img src="https://puu.sh/znVzL/d79d0c5f0c.png" alt="ss" />
Wrong. Remember the security group port thing I was talking about all the way back in step 8 when creating the EC2 Instance. Well now is where you actually specify which port to expose to the real world (to machines like our local ones).</p>

<ol>
<li>Go to Services -> EC2 -> Instances.</li>
<li>You should see the screen like the picture above.</li>
<li>Click the security groups link.
<img src="https://puu.sh/znVSA/614f86c808.png" alt="ss" /></li>
<li>Scroll down to the bottom and click the inbound tab -> Edit.</li>
<li>So if you want to expose 6379 to the real world, you need to add the port number in, and choose the source. The source is who you want to allow to access it. If you want everyone in the world to access your database, just click anywhere. (This is bad practice!) You only really want to have your database be accessed by the service that interacts with it. In other words, like an EC2 Instance that has the node server&hellip; or perhaps your local machine. Once you click save, you should now be able to send requests to PUBLIC IP:DBPORTNUMBER via postman, curls, insomnia. (You&rsquo;d change the localhost to EC2 instance IP/port on your NodeJS files).</li>
</ol>


<p>You should now be able to connect from anywhere if you specify the correct ip/port from any client! (If it doesn&rsquo;t it&rsquo;s because you need to edit the yml files for certain databases like cassandra/neo4j)&hellip; but for redis/mongo it should be straightforward&hellip;</p>

<p>Repeat the steps above for all the databases you need to create if you want to &lsquo;split them apart into different instances&rsquo;</p>

<p>-</p>

<h1>Okay I got my database deployed, but how do I deploy my app?</h1>

<p>The best way to do is to create a docker image of your app (your node files).</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>0. Install docker on your computer.
</span><span class='line'>1. Go to your root project directory
</span><span class='line'>2. $ touch dockerfile
</span><span class='line'>3. copy this into the docker file
</span><span class='line'>    FROM node:carbon
</span><span class='line'>
</span><span class='line'>    RUN mkdir -p /src/app
</span><span class='line'>
</span><span class='line'>    WORKDIR /src/app
</span><span class='line'>
</span><span class='line'>    COPY . /src/app
</span><span class='line'>
</span><span class='line'>    RUN npm install
</span><span class='line'>
</span><span class='line'>    EXPOSE 3000
</span><span class='line'>
</span><span class='line'>    CMD [ "npm", "start" ]
</span><span class='line'>4. $ docker build -t nameofproject/tag .       &lt;- make sure you include the . it builds everything inside the current directory using the dockerfile into an image.
</span><span class='line'>5. $ docker images                             &lt;- you should see your image now!
</span><span class='line'>6. Follow this guide for pushing your image to your dockerhub
</span><span class='line'>   https://docs.docker.com/docker-cloud/builds/push-images/
</span><span class='line'>7. Create a new EC2 Instance (do the steps above)
</span><span class='line'>8. Install docker on the ec2 instance
</span><span class='line'>9. Pull your docker image of your app
</span><span class='line'>10. Run the docker container and your app should now be running.
</span><span class='line'>11. Lastly, add the security group to expose whatever port you are using...
</span><span class='line'>12. Your app is now available as a service to other services by accessing the public IP!
</span><span class='line'>    This means if you have an endpoint to "/api/getUsers",  you can have a service
</span><span class='line'>    do fetch('ec2-publicip:3000/api/getUsers'). In turn, your service will call your db
</span><span class='line'>    which is set up to a different ec2 instance (you set up earlier) and if you have data,
</span><span class='line'>    return results!</span></code></pre></td></tr></table></div></figure>


<p>-</p>

<h1>Adding data to my database on docker/ec2</h1>

<p>How do I upload my data if I have .csv or .json files locally?
You can ssh them into your instances/containers.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>local-machine-$ scp -i ~/pathtomypem -r machine-image@public-ipv4:~/dst src
</span><span class='line'>ex: local-machine-$ scp -i ~/.ssh/app.pem -r ec2-user@54.159.147.19:~/. ~/.neo4j/data/import
</span><span class='line'>
</span><span class='line'>then ssh into your ec2-instance
</span><span class='line'>
</span><span class='line'>ex: local-machine-$ ssh -i ~/.ssh/app.pem ec2-user@54.159.147.19
</span><span class='line'>
</span><span class='line'>your data will be wherever you specified the dst path to.
</span><span class='line'>
</span><span class='line'>ec2-instance-$ sudo docker cp src/files image:/dist
</span><span class='line'>ex: ec2-instance-$ sudo docker cp Desktop/file da9230faeb38:/
</span><span class='line'>
</span><span class='line'>Then import your data via cli or however you did it
</span></code></pre></td></tr></table></div></figure>


<p>-</p>

<h1>Conclusion</h1>

<p>There&rsquo;s all this stuff about docker-compose I&rsquo;ve been reading about and ECS, and load balancing, and auto balancing, and clusters, and task definitions, and whatnot. These are all correct things to do and things to add on if you want to make your application &lsquo;dynamic&rsquo;.</p>

<p>A cluster is a managing system to bundle certain ec2 instances together. You can specify which specific task each service should run. Each specific task would equate to a docker container. Each service would equate to a docker image. This means you can have multiple databases running (multiple tasks) of the same database (one service). If you think about this means you can split up the work your databases do.</p>

<p>If you had two databases, you can have one specifically write, and the other read. You could create a queue that splits up the work each database does. If you have many concurrent users, you don&rsquo;t want them to read from the same database. You can create copies of your database and have certain users go to certain databases to get their info.</p>

<p>The even cooler part of AWS is the load balancing/auto balancing part. Say your ec2 instance dies, you can&rsquo;t have that happen. AWS will just create a new one based on the minimum tasks that need to be running (so no downtime).</p>

<p>The flaw of the tutorial above is that you are hardcoding a specific ec2-instance into your exxpress server. In the event, it crashes&hellip; well a new ip is assigned when you create an ec2-instance. There are ways to handle that (elastic ip addresses) or creating subnets via VPC, but I didn&rsquo;t get a chance to explore it too deeply. For the sake of our project, I don&rsquo;t think people will be creating more than 1-3 ec2 instances as well (for financial purposes).</p>

<p>This is a very <strong>NAIVE</strong> but hopefully a good way to shed light on how EC2 and docker work.</p>

<p>LASTLY, I want to say that you can be even more naive!</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>1. Push up your app image to dockerhub
</span><span class='line'>2. Create a docker-compose file of all your services.
</span><span class='line'>3. Make an EC2 Instance
</span><span class='line'>4. Install docker
</span><span class='line'>5. SSH/git clone that file into your EC2 Instance
</span><span class='line'>6. Run docker-compose file.
</span><span class='line'>
</span><span class='line'>Now ALL your containers are running on one instance (which resource-wise, very inefficient).
</span></code></pre></td></tr></table></div></figure>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[backazon - a recommendation service ]]></title>
    <link href="http://austinshin.github.io/blog/2018/01/24/amazon/"/>
    <updated>2018-01-24T16:42:45-08:00</updated>
    <id>http://austinshin.github.io/blog/2018/01/24/amazon</id>
    <content type="html"><![CDATA[<p>Project summary:
I worked on a project with a team building a backend API services system dealing with millions of data. It was to answer a business question: “Which recommendation algorithm collaboration or content-filtered, generates more sales/revenue in Amazon?”</p>

<!-- more -->


<p>Collaborative filtering is based on user-user comparison from previously viewed/bought/favorited items.</p>

<p>Content filtering is based on item-item comparison from past browsing history</p>

<p>A lot of initial planning was done as it was going to be a service integrated with 4 other services.</p>

<h2>First major decision was to decide which database to use.</h2>

<p>MYSQL vs NOSQL.</p>

<p>Should I use Postgres or Mongo or MySQL?</p>

<p>Should I use a giant big table or should I use many different entries?</p>

<p>Should I use graphQL as a querying system?</p>

<p>All of these interesting databases I was reading about sounded great until I found one: neo4j.</p>

<p>I decided to use NEO4j as a database because it was a graph database. The service I was building was only really interested in the relationships between a user and a product. Therefore, a database system that focused on relationships (a graph) which can create ‘nodes’ as users/products and ‘edges’ relationships made the most sense. This meant data modeling was clearer, data querying was more efficient vs doing multiple ‘joins’ on a table trying to create a relationship, and the database abstracted away all the complicated things. Likewise, visualization from neo4j was a plus.</p>

<p>What you sketch out on a whiteboard can be directly translated into neo4j. This was one of the best points of it.</p>

<h2>The recommendation algorithm</h2>

<p>I was particularly interested in the collaboration-filtering algorithm that Amazon used. It turns out that there are numerous ways to implement one and even combine it with a content-filtering one. I did a lot of research on machine learning and how one could transform categorical values into something comparable. This is usually done using the <strong>Pearson-correlation coefficient or cosine coefficient</strong>. After converting categorical values and representing thema s a vector, you do some linear algebra (grabbing the magnitudes) and then can transform it into a statistical value. Aftewards, one applies the k-nearest-neighbors algorithm to the two things you want to compare and you can get a list of recommended results that way.</p>

<p>I managed to implement a similarity value calculator between two different users, but for the sake of the project and simplifying it, I decided to work on the k-nearest-neighbors algorithm later. It was best to black-box the algorithm and work on the overall microservice structure first as that was the goal (dealing with optimization of queries). I definitely plan on revisiting machine learning later on (super interesting).</p>

<h2>Data generation of 40million</h2>

<p>I created a data generation script. Some of the problems I ran into were creating millions of data which was solved by generating data in batches of CSV files (JSON could have worked as well). Inserting data was done the same way.</p>

<p>A total of 10 million users, 3000 top trending product nodes were created.</p>

<p>A total of 50 million relationship types (click, wishlist, view, purchased, car) were created.</p>

<h2>Testing endpoints.</h2>

<p>Eventually I ended up implementing &lsquo;fake endpoints&rsquo; to the other services. I used my fake data to help test these results. One thing I noticed was the long query time (314ms) to return a recommended item list. I wanted to go for a much more faster, realistic speed especially when it comes to serving up millions of users at once. I wanted to aim for something below 200ms.</p>

<p>I looked up optimization methods and a couple of big words popped out:</p>

<p><strong>load balancing, horizontal/vertical scaling, sharding, caching, message bus (queues)</strong></p>

<p>In order to speed up the query time, I decided to use Redis a key-value caching database. By caching values, it optimized lookups on the most popular items to be pretty much instantaneous. The results of using redis vs not using it was night and day.</p>

<p>The end result was a 100x faster querying system going from 300ms -> 3ms.</p>

<p>The idea was to create a system where in the background at night the microservice would periodically update the cache for the most popular users. If the site was serving up someone who didn&rsquo;t come often, it&rsquo;d return a recommended list after calculations.</p>

<p><img src="https://puu.sh/zg611/931730d3d1.png" alt="stress test results" /></p>

<p>My benchmark query was 1ms. RPS: 1800~
An average query to receive some info about a user was 12ms. RPS: 1300~
An average query to receive some info about a product was 9ms. RPS: 1800~
An average query to recommend a new product was about 350ms. RPS: 44~
An average query to recommend a new product CACHED was about 6ms. RPS: 2300~</p>

<hr />

<p>An interesting thing that eventually came up was how to deploy a microservice like this to the &lsquo;real world&rsquo;.</p>

<p>One of the most popular methods was to use AWS EC2 and Docker in conjunction. A lot of research was done into this and a lot of cool things were learned. This was probably my favorite (and slightly painful) part of the project.</p>

<p>Basically you could use EC2 as a cloud computer along with Docker as a vm in order to spin up multiple instances of a certain app. It could be a db, or even my api app I created. The ability to create multiple same instances meant one could load balance. If you create multiple dbs, you could potentially shard data. All of this fell under the category of horizontally scaling ~ spreading out the work done over many instances.</p>

<p>AWS and docker was tricky to learn at first, but eventually I learned it and got my app to deploy.</p>

<h2>Below is a list of randomized notes I took while doing the project</h2>

<p>Using Redis:</p>

<p>Some questions I asked myself while thinking about and hearing about redis are&hellip;</p>

<p>What is the point of Redis?</p>

<p>What is caching?</p>

<p>What does it mean to store it in memory?</p>

<p>What makes it so much faster than storing it in a MYSQL or NOSQL database?</p>

<p>What are the tradeoffs of storing it in cache?</p>

<p>How does it work?</p>

<p>How can I use it properly?</p>

<p>How can I optimize it even more?</p>

<p>As you increase database size, so does your memory size.
<strong>useful commands</strong></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>redis-server</span></code></pre></td></tr></table></div></figure>




<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>redis-cli</span></code></pre></td></tr></table></div></figure>




<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>FLUSHDB</span></code></pre></td></tr></table></div></figure>


<p>Using Neo4J:</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ neo4j-client -u neo4j localhost</span></code></pre></td></tr></table></div></figure>


<p>localhost:7474</p>

<p>important queries:</p>

<p>&ldquo; FOR GETTING ALL RELATIONSHIPS &rdquo;</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>MATCH (a:USER)-[relatedTo]-(b) RETURN a.name, b.name, Type(relatedTo), relatedTo;</span></code></pre></td></tr></table></div></figure>


<p>&ldquo; FOR GETTING RECOMMENDED LIST FOR A SPECIFIC USER ID BASED ON WHAT OTHER USERS HAVE PURCHASED.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>"MATCH (a:USER {user_id:''})-[:RELATION]-&gt;(m)&lt;-[:RELATION]-(product),(product)-[:RELATION]-&gt;(m2) RETURN m2.name AS Recommended, count(*) AS Strength ORDER BY Strength DESC LIMIT 5;</span></code></pre></td></tr></table></div></figure>




<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>CREATE INDEX ON :Label(name);</span></code></pre></td></tr></table></div></figure>




<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>USING PERIODIC COMMIT 500
</span><span class='line'>LOAD CSV WITH HEADERS FROM "file:///events1.csv" as input
</span><span class='line'>MATCH (user:USER  {user_id: input.user_id}), (product:PRODUCT {product_id: input.product_id})
</span><span class='line'>CREATE (user)-[:RELATION { type: input.event_type }]-&gt;(product);</span></code></pre></td></tr></table></div></figure>




<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>MATCH (n:Label {name: 'Kadin Mitchell'}),
</span><span class='line'>(n)-[rel:RELATION]-&gt;(follower)
</span><span class='line'>where rel.type = 'click'
</span><span class='line'>return n, follower;</span></code></pre></td></tr></table></div></figure>




<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>MATCH (:USER {name: 'Kadin Mitchell'})-[r]-()
</span><span class='line'>RETURN r</span></code></pre></td></tr></table></div></figure>




<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>MATCH (n:Label {name: 'b'}),
</span><span class='line'>(n)-[rel:RELATION]-&gt;(follower)
</span><span class='line'>where rel.type = 'FOLLOWS'
</span><span class='line'>return n, follower</span></code></pre></td></tr></table></div></figure>


<p>Dokku
EC2
Docker
MOCHA CHAI HTTP REQUESTS (TESTS)</p>

<p>feb 1:
<a href="https://gist.github.com/austinshin/7883fe17f990fbd69425ba7fe0e2e525">https://gist.github.com/austinshin/7883fe17f990fbd69425ba7fe0e2e525</a>
-read up on ec2/aws
-docker
-set up endpoints
-set up integration between redis (caching) and neo4j
-indexing and constraints (understand what it does in neo4j)</p>

<p>things to work on:
- set up redis fetch fake request
- do a siege test on each of the endpoints.</p>

<h2>- upload to docker and try to optimize.</h2>

<p>If our client is requesting something for live time data do we need to have a SQS / message bus system set up to handle it one by one?
my guess is no!</p>

<p>QUERY SYSTEM DATA WITH ONLY DATABASE:
<a href="https://puu.sh/zfpbl/066bf7d7cb.png">https://puu.sh/zfpbl/066bf7d7cb.png</a>
need to add &ldquo;write to product&rdquo; and &ldquo;write to user&rdquo;?
 and &ldquo;create a relationship?&rdquo;
 can these just be the same thigns over and over again so we can delete it ?</p>

<p>made sure to test incrementally
to make sure everything works</p>

<p>store in redis as {user_id: recommendedList}?
or&hellip; store as index = user_id : recommendedList?</p>

<p>takes about 5 seconds to query and cache 10 results
takes about 12 seconds to query and cache 100 reuslts
takes about 30 seconds to query and cache 1000 results</p>

<p>takes 3ms to query cached recommendations.
takes 300ms to query neo4j to calculate a recommendation.</p>

<p><a href="https://puu.sh/zg611/931730d3d1.png">https://puu.sh/zg611/931730d3d1.png</a></p>

<p>TODO:
1. change content filter method to compare to products.
2. change redis key:value schema (to be {user_id: [recommended list]})
3. add a cronjob to update every so often&hellip;
4. add logic to &ldquo;check if user/product exists. if they don&rsquo;t, MERGE them into neo4j, create a relationship&rdquo; (use a queue system or something to do this? not sure&hellip;)
^ should be able to handle a good amount of new users.
5. start working on docker and getting that set up.</p>

<p>Feb 5:
refactored redis to use hash table instead of arrays~ because I needed to keep track of userid
added mocha and chai testing for TDD (there&rsquo;s a nice article about BDD, unit tests, and TDD)
created video for thesis midpoint project
<a href="https://joshldavis.com/2013/05/27/difference-between-tdd-and-bdd/">https://joshldavis.com/2013/05/27/difference-between-tdd-and-bdd/</a></p>

<p>tomorrow&rsquo;s goals:
new relic
docker
cronjob endpoints to other two services.
read up on 10k rps&hellip;
set up a messagebus queue if i need it?
unit test endpoints</p>

<p>deployment to EC2</p>

<p>work on collaborative recommendation algorithm</p>

<hr />

<p>ec2 related stuff:
<a href="https://medium.com/@andrewcbass/install-redis-v3-2-on-aws-ec2-instance-93259d40a3ce">https://medium.com/@andrewcbass/install-redis-v3-2-on-aws-ec2-instance-93259d40a3ce</a>
ssh -i ~/.ssh/redis.pem ec2-user</p>

<p>docker stuff</p>

<ol>
<li>make a dockerfile</li>
<li>add all the commands</li>
<li>docker build -t austinshin/recservice .</li>
<li>docker run -d -p 49160:3000 -v $(pwd):/src/app &ndash;name thesis austinshin/recservice</li>
<li>to remove&hellip;</li>
<li>stop the docker container</li>
<li>docker rm containername</li>
<li>docker rmi imagename
CALL dbms.changePassword(&lsquo;123&rsquo;);</li>
</ol>


<p>docker exec -t -i container_name /bin/bash</p>

<p>docker-machine ls
docker-machine start</p>

<p>docker exec -t -i recommendationservice_neo4j_1 /bin/bash
eval &ldquo;$(docker-machine env default)&rdquo;</p>

<hr />

<p>set up queue and then have it point to multiple databases.</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>ecs-cli up
</span><span class='line'>ecs-cli up --keypair app --capability-iam --size 3 --instance-type t2.medium --force
</span><span class='line'>ecs-cli compose --file docker-compose.yml up
</span><span class='line'>change security group to allow TCP 22 (with ur ip so you can ssh in)
</span><span class='line'>
</span><span class='line'>ssh -i ~/desktop/app.pem ec2-user@54.215.227.138
</span><span class='line'>sudo yum update
</span><span class='line'>sudo yum install -y docker
</span><span class='line'>sudo service docker start
</span><span class='line'>sudo usermod -a -G docker ec2-user
</span><span class='line'>docker info
</span><span class='line'>ecs-cli down</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Aircasa - A housing marketplace web app]]></title>
    <link href="http://austinshin.github.io/blog/2018/01/22/legacy/"/>
    <updated>2018-01-22T10:23:26-08:00</updated>
    <id>http://austinshin.github.io/blog/2018/01/22/legacy</id>
    <content type="html"><![CDATA[<h4>Demo</h4>

<p><a href="http://aircasa.herokuapp.com/">Try Aircasa</a></p>

<p><a href="https://github.com/hrsf87-casa/aircasa">Github</a></p>

<h5>Summary</h5>

<ul>
<li>Adopted a Minimal Viable Product of a <a href="https://github.com/airbnb-clone/airbnb">Airbnb clone</a> from a group of people who worked on it previously hence the name &lsquo;Legacy&rsquo;.</li>
<li>Familiarized self with a new code base, refactored code, added multiple new features, and deployed to heroku.</li>
</ul>


<!-- more -->


<h4>Stack</h4>

<ul>
<li>React</li>
<li>Express</li>
<li>MYSQL

<h5>Features Added</h5></li>
<li>PassportJS/Bcrypt (Login Authentication)</li>
<li>Ability to host a place</li>
<li>Track current and past bookings</li>
<li>Unique profile page</li>
<li>Uploading files utilizing Amazon Web Services</li>
<li>Fetch live data via Airbnb API</li>
<li>Styling via Bootstrap and CSS Grid</li>
<li>Provide map data via Google Maps API</li>
</ul>


<h2>The Project Phase</h2>

<p>Interesting decisions/challenges encountered.
1. Adopting a new codebase, modularity, documentation, and refactoring.
2. Learning a new technology/Methods of research
3. CSS Styling</p>

<p>This was my first time adopting a sizable codebase. Some of the things I asked myself as I was going through were
- What does this application aim to achieve? - <strong>Purpose</strong>
- How can I get this application to work so I can test its features? - <strong>Stability</strong>
- What potential features can I add to make this app stellar? - <strong>Future</strong></p>

<h5><em><strong>Modularity and Refactoring</strong></em></h5>

<p>As I was reading through it the first time, I realized the importance of documentation as well as modularity. Most of the code that should have been split up into many different files were placed in one file making it confusing and hard to understand what was going on. Functions names were out of place, styling was incorrect, and it made a lot of components that should have been simple to understand complex.</p>

<p>I always aim to code in a way that if I were to read my code 1 year from now I&rsquo;d be able to understand it. Doing so should allow others to understand it as well. So the first goal of the project immediately was to figure out what each components did, figure out why the application wasn&rsquo;t working as intended (because it was riddled with bugs), and <strong>refactor the code</strong>.</p>

<p>My teammates and I went through each file carefully dissecting which pieces were unneeded and what was needed. The first day was spent documenting and understanding how everything worked and fell in place. Afterwards, we worked to create a working application where all we had to do afterwards was to add features individually.</p>

<h5><em><strong>As a user I&rsquo;d like to be able to have a profile page that has information about me.</strong></em></h5>

<p>As the project went on I found myself working on user-facing features. The perspective of the the user was always a conscious thought. By coding in such a manner I found myself thinking about what a user would like which led to more and more features that would be nice to have (in other words, improve the user experience). Being a user of many websites myself, it was easy to think about what turns me off and makes me exit a website instantly.</p>

<p>Rather than overwhelming myself with all these &ldquo;neat features&rdquo; to include immediately, I incrementally added each nice feature to a ticketing system. Then, I worked on each one by one, keeping things modular, testing each feature as I progressed, and making sure it was just the way a user would like it. After a day&rsquo;s work, I reconvened with my team members to make sure the vision of the project was still intact. I also discussed with them if there were any other demanding features that came up that should be implemented first. <strong>Communication</strong> is key in making a project succeed. This was how most of the project was carried out afterwards.</p>

<h5><em><strong>CSS Styling</strong></em></h5>

<p>I found my biggest struggle in the project with the CSS. I&rsquo;ve previously read how arcane CSS is. I&rsquo;ve worked with template CSS bit by bit before, but this was the first time I was working from a true blank page. It was my first experience with creating a modular layout in which features can be implemented bit by bit while add styles. I&rsquo;ve used bootstrap before when working with small pieces. This time though, I wanted to undertake a bigger challenge and decided to learn the new CSS Grid system. It could have gone better, but I learned a fair amount about some of the limitations as well as the magic of CSS.</p>

<p>One of the most most important things I&rsquo;ve learned is <strong>try to plan as much of the layout as possible early.</strong>
Also, <strong>don&rsquo;t hardcode pixels. Use % and vw/vh.</strong>
One more thing, <strong>making a site with CSS from scratch is hard and takes practice.</strong> But it was fun!</p>

<h3>Wrap Up</h3>

<ul>
<li>Modularity, code documentation, proper styling leads to less confusion</li>
<li>This means when working with other or new teammembers it makes it easier on them to implement new features.</li>
<li>Fewer bugs this way too!</li>
<li>Importance of UI design (style) nearly matches functionality.</li>
<li>If it looks ugly, why would I use it even if it is functional?</li>
<li>Think from the perspective of a user always! What does the user want?</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Slackcasa - chatting web app]]></title>
    <link href="http://austinshin.github.io/blog/2018/01/22/greenfield/"/>
    <updated>2018-01-22T10:23:16-08:00</updated>
    <id>http://austinshin.github.io/blog/2018/01/22/greenfield</id>
    <content type="html"><![CDATA[<h4>Demo</h4>

<p><a href="https://slackk-casa.herokuapp.com/">Try Slackcasa</a></p>

<p><a href="https://github.com/austinshin/slackk-casa">Github</a></p>

<h5>Summary</h5>

<ul>
<li>Created a full stack web application clone from scratch of <a href="https://slack.com/">Slack</a>.</li>
<li>Worked with Trello (ticketing system), 3 other team members and implemented features for one week.</li>
</ul>


<!-- more -->


<h4>Stack</h4>

<ul>
<li>React</li>
<li>Express</li>
<li>PostGreSQL

<h5>Features Added</h5></li>
<li>Login Authentication via PassportJS</li>
<li>Passport Encryption via Bcrypt</li>
<li>Route handling via React-Router</li>
<li>Realtime Live Chat system between clients via Native JS Websockets</li>
<li>E-mail notifications via Nodemailer and CronJob</li>
<li>Unique workspaces (chatrooms) for users to join/chat</li>
<li>Styling done via Bootstrap</li>
</ul>


<h2>The Project Phase</h2>

<p>Interesting decisions/challenges encountered.
1. Creation of a web application with skills learned from before for the first time.
2. Working as a group to create a vision for the application and working to make that vision come true.
3. Creating a ticketing system.
4. Modularizing and documenting code.
5. Git Workflow
6. Websockets, Postgres, Passport</p>

<p>This was my first time working with multiple team members to create a project.
The application goal was to create an interactive, seamless, lagless environment with users connecting and chatting to each other.
My goal was to practice communicating effectively, create a modular codebase such that other members could implement features without running into merge conflicts, and learn new technologies.</p>

<h5><strong>Planning Phase</strong></h5>

<p>My team members and I spent the entire first day talking about the project. I proposed the idea we try to plan as much as possible so that we have a good sense of the big picture. By creating a vision it helps when thinking about features to implement. You can then ask yourself, does this feature work towards the vision? If not, do we need to reconvene and reevaluate our vision? From past experience, having an end goal helps keep people focused and not lose their sense of direction.</p>

<p>By the end of the day, we had multiple features to implement ticketed in the perspective of a user. <strong>As a user I&rsquo;d like to have this feature&hellip;</strong> i.e. As a user I&rsquo;d like to be able to login. This obviously led to multiple features added on top of that, but having everything written out and ticketed allows for anyone to pick up the &lsquo;ticket&rsquo; and start working towards it. Also, by separating everything out and planning most of it meant it was easy to keep things documented and modular.</p>

<p>Topics like
- &lsquo;What is our schema for pgsql going to look like?&rsquo;.
- &lsquo;What are our variables name going to be?&rsquo;
- &lsquo;What form is our data going to be sent in when communicating with the front end and back end?&rsquo;</p>

<p>We took many pictures and uploaded them for easy viewing and later reference.
It was in a sense exhausting, but felt well worth it. Everyone felt on the same page which is super key in group projects.</p>

<h4>_<strong>Implementation</strong>__</h4>

<p>It turned out that some of these technologies we were working on were new to everyone. Since everyone wanted to learn and we had four people, it made sense to do some pair programming. Adopting the driver-navigator system we proceeded to split up the work on front end and database/back end. We switched around partners to learn about websockets and how they worked on both the front and back end.</p>

<h4>_<strong>Websockets</strong>__</h4>

<p>Websockets are really cool. I&rsquo;ve worked on a chat system before but that was using native RESTFUL API where messages were updated via a setInterval. Now with websockets, everything was going to be realtime. It solves the problem of browsers being able to communicate to servers and servers to other browsers what seems to be instantly. This bi directional connection was the key to making our chat system work so it was naturally very important we implement it properly and in a way where we can reuse it when we need to implement other features.</p>

<p>We chose native websockets over sockets.io to get a better understanding of how websockets work.</p>

<h4>_<strong>PostgreSQL</strong>__</h4>

<p>Why postgres over MongoDB or MYSQL? Well, it made sense to use an ORM, but also a database in which you can join tables because of the nature of our project&rsquo;s schema. I already knew how to use MYSQL so I wanted to undertake a challenge of learning something new. The answer was PostgreSQL. It was fairly easy to pick up and intuitive to use. It shared many similarities as MYSQL (unsurprisingly).</p>

<p>As the mvp fell into place, my team members and I started to split off and work on separate features. I wanted to work on a notification system in which text and email notifications would be sent. I realized while I was planning it how much depth there would be when creating such a system. Having one type of notification system is hard enough, but add two? That&rsquo;s another layer of complexity. Then I thought about, what if you want to turn off notifications&hellip; or what if you wanted only specific notifications. I didn&rsquo;t have much time left, so I worked on what I could. Twilio or bandwidth is good to use for text notifications and I worked on it in the past. I decided to use nodemailer to send e-mail notifications and played around with cronjob to filter out certain emails and send them on a interval. It was cool (and spammy)!</p>

<h3>Wrap Up</h3>

<ul>
<li>I learned a huge deal about planning and how long it could take. The project was seemingly small, but it took a whole day. I wonder what it&rsquo;d be like in a big company with a big app.</li>
<li>Keeping things modular allows you to keep to problem small, while maintaing a vision. It means you can implement new features separately without breaking the function of the app.</li>
<li>Documentation is key because it means your team members can read and understand the code. This also means good code styling and function names.</li>
<li>As a result of point 3, our project was picked the most by others in our class to reuse for their next project.</li>
<li>Learning new technologies is always fun, and I was surprised at how good I&rsquo;ve gotten at it. I&rsquo;m aiming to get faster and better at it.</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Text me your chipotle order]]></title>
    <link href="http://austinshin.github.io/blog/2018/01/22/mvp/"/>
    <updated>2018-01-22T10:23:10-08:00</updated>
    <id>http://austinshin.github.io/blog/2018/01/22/mvp</id>
    <content type="html"><![CDATA[<p>This was my first time creating a full stack web application from scratch.</p>

<p>Stack used:
Mongo
Express
React
NodeJS</p>

<!-- more -->


<p>Some of the interesting problems I wanted to solve and understand included how webpack worked, the idea behind babel, creating a css layout, and getting functionality of my program to work.</p>

<p>Text me your chipotle order sounds exactly what it sounds like. The idea behind it is that a person wants to buy a chipotle order for his friend. The person links the friend this app. The friend goes through the app, clicking pictures of what he wants with an optional &lsquo;extra ingredients&rsquo; at the end. He inputs his friend&rsquo;s phone number which sends a botted text with the order to him.</p>

<p>It is a very simple program. The idea of using a database was to store user session information. For example, you can just automatically be logged in of some sort and work off that. One other thing I considered was using an e-mail system instead of a texting system (because people are more prone to give out their e-mails over their phone numbers simply cause you can create a fake e-mail).</p>

<p>I ended up using Twilio (over bandwidth). It was overall very simple to setup and create. As for the css layout, I went with using html/css cards. Ideally I would have liked to have created some sort of slideshow method where you scroll through and click yes or no really quickly.</p>

<p>It was a really quick (1 day) project whipped up to serve some sort of MVP. It was one of the first times I felt amazed by being able to create something from scratch really quickly with some sort of real-life contribution (as dumb as it sounds).</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[hello]]></title>
    <link href="http://austinshin.github.io/blog/2015/11/18/hello/"/>
    <updated>2015-11-18T22:32:36-08:00</updated>
    <id>http://austinshin.github.io/blog/2015/11/18/hello</id>
    <content type="html"><![CDATA[<p>Current todo list:</p>

<p>enigami</p>
]]></content>
  </entry>
  
</feed>

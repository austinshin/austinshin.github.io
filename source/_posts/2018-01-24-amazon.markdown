---
layout: post
title: "backazon - a recommendation service "
date: 2018-01-24 16:42:45 -0800
comments: true
categories: sideproject
---


Project summary:
I worked on a project with a team building a backend API services system dealing with millions of data. It was to answer a business question: “Which recommendation algorithm collaboration or content-filtered, generates more sales/revenue in Amazon?”

<!-- more -->

Collaborative filtering is based on user-user comparison from previously viewed/bought/favorited items.

Content filtering is based on item-item comparison from past browsing history

A lot of initial planning was done as it was going to be a service integrated with 4 other services.

## First major decision was to decide which database to use.

MYSQL vs NOSQL.

Should I use Postgres or Mongo or MySQL?

Should I use a giant big table or should I use many different entries?

Should I use graphQL as a querying system?

All of these interesting databases I was reading about sounded great until I found one: neo4j.

I decided to use NEO4j as a database because it was a graph database. The service I was building was only really interested in the relationships between a user and a product. Therefore, a database system that focused on relationships (a graph) which can create ‘nodes’ as users/products and ‘edges’ relationships made the most sense. This meant data modeling was clearer, data querying was more efficient vs doing multiple ‘joins’ on a table trying to create a relationship, and the database abstracted away all the complicated things. Likewise, visualization from neo4j was a plus.

What you sketch out on a whiteboard can be directly translated into neo4j. This was one of the best points of it.

## Data generation of 40million
I created a data generation script. Some of the problems I ran into were creating millions of data which was solved by generating data in batches of CSV files (JSON could have worked as well). Inserting data was done the same way.

A total of 10 million users, 3000 top trending product nodes were created.

A total of 50 million relationship types (click, wishlist, view, purchased, car) were created.

## Testing endpoints.

Eventually I ended up implementing 'fake endpoints' to the other services. I used my fake data to help test these results. One thing I noticed was the long query time (314ms) to return a recommended item list. I wanted to go for a much more faster, realistic speed especially when it comes to serving up millions of users at once. I wanted to aim for something below 200ms.

I looked up optimization methods and a couple of big words popped out:

**load balancing, horizontal/vertical scaling, sharding, caching, message bus (queues)**

In order to speed up the query time, I decided to use Redis a key-value caching database. By caching values, it optimized lookups on the most popular items to be pretty much instantaneous. The results of using redis vs not using it was night and day.

The end result was a 100x faster querying system going from 300ms -> 3ms.

The idea was to create a system where in the background at night the microservice would periodically update the cache for the most popular users. If the site was serving up someone who didn't come often, it'd return a recommended list after calculations.

![stress test results](https://puu.sh/zg611/931730d3d1.png)

My benchmark query was 1ms. RPS: 1800~
An average query to receive some info about a user was 12ms. RPS: 1300~
An average query to receive some info about a product was 9ms. RPS: 1800~
An average query to recommend a new product was about 350ms. RPS: 44~
An average query to recommend a new product CACHED was about 6ms. RPS: 2300~


---
An interesting thing that eventually came up was how to deploy a microservice like this to the 'real world'.

One of the most popular methods was to use AWS EC2 and Docker in conjunction. A lot of research was done into this and a lot of cool things were learned. This was probably my favorite (and slightly painful) part of the project.

Basically you could use EC2 as a cloud computer along with Docker as a vm in order to spin up multiple instances of a certain app. It could be a db, or even my api app I created. The ability to create multiple same instances meant one could load balance. If you create multiple dbs, you could potentially shard data. All of this fell under the category of horizontally scaling ~ spreading out the work done over many instances.

AWS and docker was tricky to learn at first, but eventually I learned it and got my app to deploy.

## Below is a list of randomized notes I took while doing the project
Using Redis:

Some questions I asked myself while thinking about and hearing about redis are...

What is the point of Redis?

What is caching?

What does it mean to store it in memory?

What makes it so much faster than storing it in a MYSQL or NOSQL database?

What are the tradeoffs of storing it in cache?

How does it work?

How can I use it properly?

How can I optimize it even more?

As you increase database size, so does your memory size.
**useful commands**

```
redis-server
```

```
redis-cli
```

```
FLUSHDB
```

Using Neo4J:

```
$ neo4j-client -u neo4j localhost
```

localhost:7474

important queries:

" FOR GETTING ALL RELATIONSHIPS "

```
MATCH (a:USER)-[relatedTo]-(b) RETURN a.name, b.name, Type(relatedTo), relatedTo;
```

" FOR GETTING RECOMMENDED LIST FOR A SPECIFIC USER ID BASED ON WHAT OTHER USERS HAVE PURCHASED.
```
"MATCH (a:USER {user_id:''})-[:RELATION]->(m)<-[:RELATION]-(product),(product)-[:RELATION]->(m2) RETURN m2.name AS Recommended, count(*) AS Strength ORDER BY Strength DESC LIMIT 5;
```

```
CREATE INDEX ON :Label(name);
```

```
USING PERIODIC COMMIT 500
LOAD CSV WITH HEADERS FROM "file:///events1.csv" as input
MATCH (user:USER  {user_id: input.user_id}), (product:PRODUCT {product_id: input.product_id})
CREATE (user)-[:RELATION { type: input.event_type }]->(product);
```

```
MATCH (n:Label {name: 'Kadin Mitchell'}),
(n)-[rel:RELATION]->(follower)
where rel.type = 'click'
return n, follower;
```

```
MATCH (:USER {name: 'Kadin Mitchell'})-[r]-()
RETURN r
```

```
MATCH (n:Label {name: 'b'}),
(n)-[rel:RELATION]->(follower)
where rel.type = 'FOLLOWS'
return n, follower
```
Dokku
EC2
Docker
MOCHA CHAI HTTP REQUESTS (TESTS)

feb 1:
https://gist.github.com/austinshin/7883fe17f990fbd69425ba7fe0e2e525
-read up on ec2/aws
-docker
-set up endpoints
-set up integration between redis (caching) and neo4j
-indexing and constraints (understand what it does in neo4j)

things to work on:
- set up redis fetch fake request
- do a siege test on each of the endpoints.
- upload to docker and try to optimize.
-

If our client is requesting something for live time data do we need to have a SQS / message bus system set up to handle it one by one?
my guess is no!

QUERY SYSTEM DATA WITH ONLY DATABASE:
https://puu.sh/zfpbl/066bf7d7cb.png
need to add "write to product" and "write to user"?
 and "create a relationship?"
 can these just be the same thigns over and over again so we can delete it ?


made sure to test incrementally
to make sure everything works

store in redis as {user_id: recommendedList}?
or... store as index = user_id : recommendedList?

takes about 5 seconds to query and cache 10 results
takes about 12 seconds to query and cache 100 reuslts
takes about 30 seconds to query and cache 1000 results

takes 3ms to query cached recommendations.
takes 300ms to query neo4j to calculate a recommendation.

https://puu.sh/zg611/931730d3d1.png


TODO:
1. change content filter method to compare to products.
2. change redis key:value schema (to be {user_id: [recommended list]})
3. add a cronjob to update every so often...
4. add logic to "check if user/product exists. if they don't, MERGE them into neo4j, create a relationship" (use a queue system or something to do this? not sure...)
^ should be able to handle a good amount of new users.
5. start working on docker and getting that set up.

Feb 5:
refactored redis to use hash table instead of arrays~ because I needed to keep track of userid
added mocha and chai testing for TDD (there's a nice article about BDD, unit tests, and TDD)
created video for thesis midpoint project
https://joshldavis.com/2013/05/27/difference-between-tdd-and-bdd/

tomorrow's goals:
new relic
docker
cronjob endpoints to other two services.
read up on 10k rps...
set up a messagebus queue if i need it?
unit test endpoints

deployment to EC2

work on collaborative recommendation algorithm


-------------
ec2 related stuff:
https://medium.com/@andrewcbass/install-redis-v3-2-on-aws-ec2-instance-93259d40a3ce
ssh -i ~/.ssh/redis.pem ec2-user

docker stuff

1. make a dockerfile
2. add all the commands
3. docker build -t austinshin/recservice .
4. docker run -d -p 49160:3000 -v $(pwd):/src/app --name thesis austinshin/recservice
5. to remove...
6. stop the docker container
7. docker rm containername
8. docker rmi imagename
CALL dbms.changePassword('123');


docker exec -t -i container_name /bin/bash

docker-machine ls
docker-machine start

docker exec -t -i recommendationservice_neo4j_1 /bin/bash
eval "$(docker-machine env default)"

----------------------------------
set up queue and then have it point to multiple databases.

```
ecs-cli up
ecs-cli up --keypair app --capability-iam --size 3 --instance-type t2.medium --force
ecs-cli compose --file docker-compose.yml up
change security group to allow TCP 22 (with ur ip so you can ssh in)

ssh -i ~/desktop/app.pem ec2-user@54.215.227.138
sudo yum update
sudo yum install -y docker
sudo service docker start
sudo usermod -a -G docker ec2-user
docker info
ecs-cli down
```
